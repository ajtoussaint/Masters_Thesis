{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8035f3-19e3-48ce-bcd1-367296b27a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.ASSISTments2009.assist09_wrangler as assist09_wrangler\n",
    "import data.ASSISTments2012.assist12_wrangler as assist12_wrangler\n",
    "import data.junyi2015.junyi15_wrangler as junyi15_wrangler\n",
    "import data.NeurIPS2020.neur20_wrangler as neur20_wrangler\n",
    "import data.dataSplitter as ds\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d62abc-e40f-48c6-ab4c-1233798a1914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select a model to test:\n",
      "0.all\n",
      "1.['IRT']\n",
      "2.['NCDM']\n",
      "3.['KaNCD']\n",
      "4.['CAKE']\n",
      " 3\n",
      "Select a dataset to test on:\n",
      "0.all\n",
      "1.['ASSIST09']\n",
      "2.['NEUR20']\n",
      "3.['JUNYI15']\n",
      "4.['ASSIST12']\n",
      " 1\n",
      "Select a run condition:\n",
      "0.all\n",
      "1.['basic']\n",
      "2.['sampled']\n",
      "3.['undersampleEven']\n",
      "4.['undersampleCorrect']\n",
      "5.['undersampleIncorrect']\n",
      "6.['correctSaturated']\n",
      "7.['incorrectSaturated']\n",
      " 1\n",
      "Collect user data?\n",
      " n\n"
     ]
    }
   ],
   "source": [
    "from models import IRT, NCDM, KaNCD, CAKE\n",
    "\n",
    "paths = {\n",
    "    \"ASSIST09\" : \"data/ASSISTments2009/\",\n",
    "    \"NEUR20\" : \"data/NeurIPS2020/\",\n",
    "    \"JUNYI15\" : \"data/junyi2015/\",\n",
    "    \"ASSIST12\": \"data/ASSISTments2012/\"\n",
    "}\n",
    "\n",
    "modelList ={\n",
    "    \"0\":[\"IRT\", \"NCDM\", \"KaNCD\", \"CAKE\"],\n",
    "    \"1\":[\"IRT\"],\n",
    "    \"2\":[\"NCDM\"],\n",
    "    \"3\":[\"KaNCD\"],\n",
    "    \"4\":[\"CAKE\"]\n",
    "}\n",
    "\n",
    "datasetList = {\n",
    "    \"0\":[\"ASSIST09\", \"NEUR20\", \"JUNYI15\", \"ASSIST12\"],\n",
    "    \"1\":[\"ASSIST09\"],\n",
    "    \"2\":[\"NEUR20\"],\n",
    "    \"3\":[\"JUNYI15\"],\n",
    "    \"4\":[\"ASSIST12\"]\n",
    "}\n",
    "\n",
    "runTypeList = {\n",
    "    \"0\":[\"basic\", \"sampled\", \"undersampleEven\", \"undersampleCorrect\", \"undersampleIncorrect\"],\n",
    "    \"1\":[\"basic\"],\n",
    "    \"2\":[\"sampled\"],\n",
    "    \"3\":['undersampleEven'],\n",
    "    \"4\":[\"undersampleCorrect\"],\n",
    "    \"5\":[\"undersampleIncorrect\"],\n",
    "    \"6\":[\"correctSaturated\"],\n",
    "    \"7\":[\"incorrectSaturated\"],\n",
    "    \n",
    "}\n",
    "\n",
    "modelFuncs = {\n",
    "    \"IRT\": IRT.run_IRT,\n",
    "    \"NCDM\":NCDM.run_NCDM,\n",
    "    \"KaNCD\":KaNCD.run_KaNCD,\n",
    "    \"CAKE\":CAKE.run_CAKE\n",
    "}\n",
    "models = modelList[input(f\"Select a model to test:\\n\" + \"\\n\".join([f'{k}.{modelList[k] if len(modelList[k]) == 1 else \"all\"}' for k in modelList.keys()]) + \"\\n\")]\n",
    "datasets = datasetList[input(f\"Select a dataset to test on:\\n\" + \"\\n\".join([f'{k}.{datasetList[k] if len(datasetList[k]) == 1 else \"all\"}' for k in datasetList.keys()]) + \"\\n\")]\n",
    "runTypes = runTypeList[input(f\"Select a run condition:\\n\" + \"\\n\".join([f'{k}.{runTypeList[k] if len(runTypeList[k]) == 1 else \"all\"}' for k in runTypeList.keys()]) + \"\\n\")]\n",
    "user_data = True if input(\"Collect user data?\\n\") == 'y' else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0192634e-bbeb-4a9e-b4c0-de75136a1bfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data exists in result file\n"
     ]
    }
   ],
   "source": [
    "results_path = \"results.csv\"\n",
    "\n",
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        for runType in runTypes:\n",
    "            #if the csv doesn't exist create it\n",
    "            if not os.path.exists(results_path):\n",
    "                df = pd.DataFrame(columns=[\n",
    "                    \"model\", \"runType\", \"dataset\",\n",
    "                    \"test_correct_ratio\", \"test_correct_ratio_raw\",\n",
    "                    \"train_correct_ratio\", \"train_correct_ratio_raw\",\n",
    "                    \"n_train_examples\", \"n_train_examples_raw\",\n",
    "                    \"n_test_examples\", \"n_test_examples_raw\",\n",
    "                    \"n_valid_examples\", \"n_valid_examples_raw\",\n",
    "                    \"n_knowledge_concepts\", \"n_problems\",\n",
    "                    \"n_users\", \"n_users_raw\",\n",
    "                    \"ACC\", \"ACC_raw\", \"AUC\", \"AUC_raw\",\n",
    "                    \"MAE\", \"MAE_raw\", \"RMSE\", \"RMSE_raw\",\n",
    "                    \"ACC_std\", \"ACC_std_raw\", \"AUC_std\", \"AUC_std_raw\",\n",
    "                    \"MAE_std\", \"MAE_std_raw\", \"RMSE_std\", \"RMSE_std_raw\",\n",
    "                    \"avg_train_duration\", \"avg_train_duration_raw\",\n",
    "                    \"performed_at\"\n",
    "                ])\n",
    "            else:\n",
    "                #pull in the csv as a dataframe\n",
    "                df = pd.read_csv(results_path)\n",
    "            #check the presence of an existing run\n",
    "            if (((df['model'] == model) & (df['runType'] == runType) & (df['dataset'] == dataset)).any()) and not user_data:\n",
    "                #data entry already exists, skip\n",
    "                print(\"data exists in result file\")\n",
    "            else:\n",
    "                Q, data = ds.prepareData(dataset = dataset, runType = runType)\n",
    "                #find average training correct/incorrect ratio per student (should be ~0.50)\n",
    "                avg_train_score = []\n",
    "                avg_test_score = []\n",
    "                avg_valid_score = []\n",
    "                avg_train_size = []\n",
    "                avg_test_size = []\n",
    "                avg_valid_size = []\n",
    "                avg_n_users = []\n",
    "                for d in data:\n",
    "                    avg_train_score.append(d[\"train\"].groupby('user_id')[\"score\"].mean().mean())\n",
    "                    avg_test_score.append(d[\"test\"].groupby('user_id')[\"score\"].mean().mean())\n",
    "                    avg_valid_score.append(d[\"valid\"].groupby('user_id')[\"score\"].mean().mean())\n",
    "                    avg_train_size.append(len(d[\"train\"]))\n",
    "                    avg_test_size.append(len(d[\"test\"]))\n",
    "                    avg_valid_size.append(len(d[\"valid\"]))\n",
    "                    avg_n_users.append(len(d[\"train\"][\"user_id\"].unique()))\n",
    "                \n",
    "                result_object = {\n",
    "                    \"model\" : model,\n",
    "                    \"runType\" : runType,\n",
    "                    \"dataset\" : dataset,\n",
    "                    \"test_correct_ratio\":np.mean(avg_test_score),\n",
    "                    \"test_correct_ratio_raw\":avg_test_score,\n",
    "                    \"train_correct_ratio\":np.mean(avg_train_score),\n",
    "                    \"train_correct_ratio_raw\":avg_train_score,\n",
    "                    \"n_train_examples\":np.mean(avg_train_size),\n",
    "                    \"n_train_examples_raw\":avg_train_size,\n",
    "                    \"n_test_examples\":np.mean(avg_test_size),\n",
    "                    \"n_test_examples_raw\":avg_test_size,\n",
    "                    \"n_valid_examples\":np.mean(avg_valid_size),\n",
    "                    \"n_valid_examples_raw\":avg_valid_size,\n",
    "                    \"n_knowledge_concepts\":len(Q.columns) - 1,\n",
    "                    \"n_problems\":len(Q),\n",
    "                    \"n_users\":np.mean(avg_n_users),\n",
    "                    \"n_users_raw\":avg_n_users,\n",
    "                }\n",
    "                \n",
    "                accs, aucs, maes, rmses, times = [], [], [], [], []\n",
    "                \n",
    "                #run each provided data configuration and collect statistics\n",
    "                \n",
    "                for n, run in enumerate(data): \n",
    "                    \n",
    "                    start_timer = time.time()\n",
    "                    acc, auc, mae, rmse = modelFuncs[model](Q, run[\"train\"], run[\"test\"], run[\"valid\"], n, dataset, runType, user_data = user_data)\n",
    "                    end_timer = time.time()\n",
    "                    \n",
    "                    accs.append(acc)\n",
    "                    aucs.append(auc)\n",
    "                    maes.append(mae)\n",
    "                    rmses.append(rmse)\n",
    "                    times.append(end_timer - start_timer)\n",
    "\n",
    "                    print(f\"\\nRan fold {n} on {model} with {dataset} and run type {runType}\\n\")\n",
    "\n",
    "                #collecting user data will slow the train time\n",
    "                if not user_data:\n",
    "                    #format the output\n",
    "                    result_object['ACC'] = np.mean(accs)\n",
    "                    result_object['ACC_raw'] = accs\n",
    "                    result_object['AUC'] = np.mean(aucs)\n",
    "                    result_object['AUC_raw'] = aucs\n",
    "                    result_object['MAE'] = np.mean(maes)\n",
    "                    result_object['MAE_raw'] = maes\n",
    "                    result_object['RMSE'] = np.mean(rmses)\n",
    "                    result_object['RMSE_raw'] = rmses\n",
    "                    result_object['ACC_std'] = np.std(accs)\n",
    "                    result_object['ACC_std_raw'] = accs\n",
    "                    result_object['AUC_std'] = np.std(aucs)\n",
    "                    result_object['AUC_std_raw'] = aucs\n",
    "                    result_object['MAE_std'] = np.std(maes)\n",
    "                    result_object['MAE_std_raw'] = maes\n",
    "                    result_object['RMSE_std'] = np.std(rmses)\n",
    "                    result_object['RMSE_std_raw'] = rmses\n",
    "                    result_object['avg_train_duration'] = np.mean(times)\n",
    "                    result_object['avg_train_duration_raw'] = times\n",
    "                    result_object['performed_at'] = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    df = pd.concat([df, pd.DataFrame([result_object])], ignore_index=True)\n",
    "                    \n",
    "                    df.to_csv(results_path, index=False)\n",
    "                \n",
    "                print(f\"\\nVVVVVVVVVV\\nRan {model} on {dataset} type {runType}\\n^^^^^^^^^^\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b6acbc-ff24-4a51-b2bf-7f00159dfb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
